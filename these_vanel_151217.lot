\babel@toc {english}{}
\addvspace {10\p@ }
\contentsline {table}{\numberline {1.1}{\ignorespaces Temporal-Proximity-Based Clustering Approach\relax }}{22}{table.caption.34}
\contentsline {table}{\numberline {1.2}{\ignorespaces Representation-Based Clustering Approach Paper\relax }}{23}{table.caption.35}
\contentsline {table}{\numberline {1.3}{\ignorespaces Model-Based Clustering Approach\relax }}{24}{table.caption.36}
\addvspace {10\p@ }
\contentsline {table}{\numberline {2.1}{\ignorespaces Classification errors associated with the number of segments \textbf {N} chosen by the heuristics IDDTW and FDTW. When two numbers of segments $N_1$ and $N_2$ are associated with the same classification error, the smallest is considered. The classification error is calculated based on the 3 fold cross validation applied on the training set.\relax }}{44}{table.2.1}
\contentsline {table}{\numberline {2.2}{\ignorespaces Comparison of generalization errors. In \textbf {italics}, the smallest generalization error. In \textbf {bold}, the smallest generalization error between IDDTW and FDTW. \textbf {N }is the number of segments selected and $\ell $ is the number of data points in a segment $(l = \delimiter "4262304 \frac {n}{N}\delimiter "5263305 )$. The generalization error is computed on the testing set. Note : DTW (r) is a constraint version of DTW where the number of consecutive data points that can be compared to a single point during the warping is bounded. r represents the size of the warping windows\relax }}{46}{table.2.2}
\addvspace {10\p@ }
\contentsline {table}{\numberline {3.1}{\ignorespaces Datasets.\relax }}{68}{table.caption.84}
\contentsline {table}{\numberline {3.2}{\ignorespaces Comparison of the Rand Index of SUSH (RI\_SUSh) and FOTS-SUSh (RI\_FOTS). The best Rand Index is in bold\relax }}{69}{table.caption.85}
\contentsline {table}{\numberline {3.3}{\ignorespaces Comparison between k-Shape, USLM and FOTS-SUShapelet.\relax }}{70}{table.caption.86}
\addvspace {10\p@ }
\contentsline {table}{\numberline {4.1}{\ignorespaces Straight displacement in manual wheelchair\relax }}{87}{table.4.1}
\contentsline {table}{\numberline {4.2}{\ignorespaces Asymmetry (Edit distance) of subjects' propulsion with regard to their number of years of practice.\relax }}{87}{table.caption.110}
\contentsline {table}{\numberline {4.3}{\ignorespaces Anthropometric and physiological characteristics of the subjects who participated in this study.\relax }}{89}{table.caption.111}
\contentsline {table}{\numberline {4.4}{\ignorespaces Occurrence frequency of each cycle type in each subject's displacements.\relax }}{90}{table.caption.112}
\contentsline {table}{\numberline {4.5}{\ignorespaces Similarity matrix between all wheelchair users.\relax }}{91}{table.caption.113}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {table}{\numberline {B.1}{\ignorespaces 85 UCR datasets used for experimental validation. The full list is available in \cite {UCRArchive}\relax }}{119}{table.B.1}
\contentsline {table}{\numberline {B.2}{\ignorespaces Column \textbf {(1)} presents \textbf {numbers} of the datasets. Column \textbf {(2)} the \textbf {reduced length} of the time series. Column \textbf {(3)} is the \textbf {ratio} of the length of the reduced time series over the length of the initial time series. Column \textbf {(4)} designates the \textbf {1-Nearest Neighbor} algorithm, associated to the \textbf {Euclidean distance}. Column \textbf {(5)} designates the algorithm of \textbf {1- Nearer Neighbor}, associated with the algorithm of \textbf {dynamic dynamic temporal deformation} using a \textbf {constraint} called deformation window that allows to stop the comparison of time series when one perceives that they are very different. Column \textbf {(6)} represents \textbf {1-Nearest Neighbor} algorithm associated to the \textbf {unconstrained dynamic time warping} applied to the \textbf {raw data}. Column \textbf {(7)} represents the \textbf {algorithm. 1-Nearest Neighbor} associated with the \textbf {dynamic time warping algorithm without constraints}, applied on the \textbf {compact representations} produced by our algorithm. We firstly compare, the classification error of the algorithms of columns(6) and (7) the smallest error is in \textbf {bold}. Then we compare the classification errors of algorithms of columns(4), (5), (6) and (7) the smallest error is put \textbf {italics}.\relax }}{123}{table.B.2}
\addvspace {10\p@ }
\contentsline {table}{\numberline {C.1}{\ignorespaces descriptive statistic\relax }}{125}{table.caption.150}
\contentsline {table}{\numberline {C.2}{\ignorespaces normality test show that residues (uncertainty) do not follow normal law.\relax }}{127}{table.caption.155}
\addvspace {10\p@ }
\contentsline {table}{\numberline {D.1}{\ignorespaces Evolution of manual wheelchair propulsion technique with training\relax }}{133}{table.D.1}
\babel@toc {english}{}
