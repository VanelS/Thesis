\chapter[Knowledge discovery on time series]{Knowledge discovery on time series:Background}
\label{kdd}
%\minitoc

\section{Introduction}

Datasets can be grouped into four main categories regarding their temporality \cite{roddick2002survey}: 
\begin{itemize}
\item Static datasets: these are datasets with no temporal context. We have for example the radius of a wheel, the circumference of a circle, the gravity in a place.
\item Sequences datasets: they consist of   ordered sequences of events. This category includes an order but not time. As an example, we can cite a DNA sequence (GTTTTCCCAGTCACGAC).
\item Time-indexed datasets: they consist of a set of temporal data sequences ; for example a set of measures taken at a more or less regular time intervals.
\item Full-time data: Each tuple has one or more time components; time series belongs to this latter category.
\end{itemize}

Time series have several characteristic properties: usually, they are noisy, uncertain and  
they often have high dimensionality and high auto-correlation. Each of those features can interfere with the mining of time series.  To remedy this, preprocessing technics have been proposed in the literature.
\section{Preprocessing of time series}


\subsection{Denoising time series}
Several filters have been proposed in the literature to remove the noise contained in time series. In this section, the most  frequently used filters are presented.
\paragraph{Kernel smoothing:} this filter refers to a statistical technique for recovery of underlying structure in data sets. Its basic principle is to estimate a real-valued function as the weighted average of neighboring observed data. The weight is defined by a function named kernel, such that closer points to real values are given higher weights \cite{wand1994kernel}.

\paragraph{Polynomial Regression:} this filter consists in fitting a nonlinear relationship between the values of an independent variable x (predictor variable)  and the corresponding conditional mean of y (variable to explain), denoted E(y |x). This filter has been used to describe nonlinear phenomena. More formally, polynomial regression is defined as the problem of finding a polynomial:  $g(x)=\beta_{0}+\beta_{1}x+...+\beta_{m}x^{m}$ of a certain degree $m$ for wich $E(Y-g(x))^{2}$ is as small as possible \cite{kendall1961advanced}.

\paragraph{Wiener-Kolmogorov Filtering of Short Stationary Sequences:}  
The idea of this filter is to produce a statistical estimate of the actual signal from the noisy signal. Using the Wiener-Kolmogorov filter assumes the knowledge of stationary signal, noise spectra, and additive noise \cite{pollock2007wiener}.

\paragraph{Filtering in the Frequency Domain:}   The purpose of frequency-based filters is to remove the noise contained in a signal. To achieve this goal, the signal is initially broken down into a set of frequencies using a Fourier transform. This set of frequencies is called the signal spectrum. Depending on the application, it may be appropriate to suppress high or low frequencies, or both, in order to remove signal noise. These filters are generally named low-pass, high-pass, bandpass, or notch filter. These filters can also be combined in many ways: in cascade, in parallel, etc \cite{buttkus2012spectral}.

\paragraph{Kalman Filter and the Smoothing Algorithm,} also known as linear quadratic estimation (LQE), is a Bayesian estimation technique used to track stochastic dynamic systems being observed with noisy sensors. The filter produces estimates of unknown variables that tend to be more accurate than those based on a single measurement alone, by estimating a joint probability distribution over the variables for each timeframe. The algorithm works in two phases: extrapolation (prediction) and update (correction). In the extrapolation step, the Kalman filter produces estimates of the current state variables, along with their uncertainties, based on the previous state variables and their uncertainties. Once the outcome of the next measurement is observed, these estimates are updated using a weighted average, with a higher weight being given to estimates with higher certainty. The algorithm is recursive. It can run in real time, using only the current input measurements and the previously calculated state and its uncertainty matrix \cite{matthies1989kalman}.



\subsection{Reducing uncertainty}
Another important step of preprocessing time series is to reduce the uncertainty that they contained. For this purpose some transformations have been introduced in literature.


\paragraph{Uncertain moving average:}   For uncertain time series, each value is associated with a standard deviation representing uncertainty. The uncertain moving average (UMA) filter is then defined as the weighted average of the consecutive data points of a time series over a given time interval. The weights at each timestamp $i$ are calculated from the inverse of the uncertainty. Thus, in the calculation of the mean, a weight $(w)$ inversely proportional to the uncertainty will be given to each data point in the time series. Uncertain moving average returns times series \cite{Orang2015}: 
\begin{eqnarray}
x^{UMA}=<x_{1}^{UMA},...,x_{m}^{UMA}>
\end{eqnarray}
for which
\begin{eqnarray}
x_{i}^{UMA}=\frac{1}{2w+1}\stackrel[k=i-w]{i+w}{\sum}\frac{x_{k}}{\sigma_{k}},\,1\leq i\leq m.
\end{eqnarray}

Where $\sigma_{k}$ are uncertainty associated with data points $x_{k}$.

\paragraph{Z-normalization} is used with uncertain moving average to reduce the advert effect of uncertainty in time series. In general, z-normalization improves similarity search quality, because it makes similarity measures invariant to scaling and shifting. Given an uncertain time series: 
\begin{eqnarray}
X=<X_{1},...,X_{m}>,
\end{eqnarray}

its normal form: 
\begin{eqnarray}
\hat{X}=<\hat{X}_{1},...,\hat{X}_{m}>
\end{eqnarray}

is defined as follows: 

\begin{eqnarray}
\ensuremath{\hat{X}_{i}}=\frac{X_{i}-\overline{X}}{S_{X}},
\end{eqnarray}

where $\overline{X}$ and $S_{X}$ denote the sample mean and standard deviation of expected values of X, respectively \cite{Orang2015}. That is,

\begin{eqnarray}
\overline{X}=\frac{1}{n}\stackrel[i=1]{n}{\sum}E(X_{i}),
\end{eqnarray}
\begin{eqnarray}
S_{X}=\sqrt{\frac{1}{(n-1)}\stackrel[i=1]{n}{\sum}(E(X_{i})-\overline{X})^{2}}.
\end{eqnarray}


\subsection{Dimensionality reduction}
Time complexity of a mining time series algorithm depends on the length of the time series. Reducing dimensionality of time series allows reducing their processing time. To achieve this goal, many representations have been proposed and can be grouped into three main categories: 

\paragraph{Non-data-adaptive:}Dimension reduction methods are called non-data-adaptive because they take parameters of which value does not vary according to the considered data set. One of the first work in this family was done by Agrawal \\ \cite{Agrawal1993} who used a Discrete Fourier Transform to compress time series. In the same family, we can also cite the following time series representations:  Discrete Wavelet Transform (DWT) \cite{chan1999efficient}, Piecewise Linear Approximation (PLA) \cite{eriksson2004piecewise}, Piecewise Aggregate Approximation (PAA) \cite{Keogh2001a}. 

\paragraph{Data adaptive:} This family of time series representation  consists of methods that take the properties of the dataset into account when choosing the method parameters. All non-data-adaptive representations can be transformed into data-adaptive representations by adding a parameter selection method to them. As examples of data-adaptive representations, there are Adaptive Piecewise Constant Approximation (APCA) \cite{keogh2001locally}, Singular Value Decomposition (SVD) \cite{de1994singular} and Symbolic Aggregate Approximation (SAX) \cite{lin2003symbolic}.

\paragraph{Model based:} The assumption here is that time series are described by an underlying model.  Dimensionality reduction is achieved by identifying the model parameters that generate the time series. Several approaches use temporal parametric models such as statistical modeling by feature extraction \cite{Esling2012}, Auto Regressive Moving Average (ARMA) models \cite{kalpakis2001distance}, Markov Chains (MCs) and Hidden Markov Models (HMM) \cite{panuccio2002hidden}.
\\

After cleaning the time series, we are now ready to extract relevant information from them. Several data mining tasks can be performed on time series.

\section{Similarity Measures}

Before performing data mining tasks, it is essential to be able to compare time series. Most often,  similarity functions compare time series as humans would do. Indeed, without much of stretch, human recognition understands and looks at the likenesses between two time series based on their amplitude, scale, temporal warping, noise, and outliers. As indicated by \cite{fu2011review}  \cite{ralanamahatana2005mining}  \cite{Esling2012}, any similarity measure  for time series comparison ought to be reliable with human recognition and perception and have the following properties:


\begin{itemize}
\item It should perceive perceptually comparative datasets even if  they are not mathematically identical;
\item It should resemble human intuition;
\item It should be able to capture global and local similarities;
\item It should have a universal meaning that is not restricted to a particular type of time series datasets and do not assume some constraints on time series data;
\item It should be robust to distortions and set of transformations. More specifically it should be robust to amplitude
shifting, uniform amplification, uniform time scaling, dynamic amplification, dynamic time scaling, adding noise and outliers transformations or any combination of these transformations.
\end{itemize}
The latter property is also known as invariance.

\subsection{Time-Series invariances}
In this section, we briefly review common time-series distortions and their invariances. More detailed information can be found in \cite{batista2014cid}. 

\paragraph{Scaling and translation invariances:} We should be able to perceive the similarity of sequences in spite of contrasts in amplitude (scaling) and offset (translation). For instance, these invariances may be helpful to analyze seasonal variations in currency values on foreign trade markets without being biased by inflation.


\paragraph{Shift invariance:}  We should be able to recognize two similar sequences even if they vary in phase (global alignment) or when there are regions of the sequences that are aligned and others are not (local alignment). For instance, heartbeats can be out of phase depending on when we start recording, and handwritings of the same sentence from various people will require alignment depending on the size of the letters and on the spaces between words (local alignment).


\paragraph{Uniform scaling invariance:} We should be able to compare two sequences even if they have different lengths. To do so, sequences that differ in length require either extending of the shorter sequence or, contracting of the longer sequence. For instance, this invariance is required for  heartbeats recorded at different sample frequencies  (e.g.: 10, 50 ou 100 Hz).  

\paragraph{Occlusion invariance:} We should be able to compare two time series even if some of their sub-sequences  are missing; we can also compare the sequences by ignoring the sub-sequences that do not match well.  For example, suppose an archaeologist who has just found a skull in a research site, and  would like to determine to which species this skull belongs. Let us also suppose that we have a database of time series corresponding to the skulls of living species. We could then compare the time series from the found skull to those stored in the database. This comparison should be possible even if the found skull is damaged. In other words, we should be able to make a comparison even if the time series extracted from the found skull has missing sub-sequences.

\paragraph{Complexity invariance:} We should be able to recognize time series with similar shape even if they have different complexity.  For example, the same audio signals that were recorded indoors and outdoors might be considered similar, although outdoor signals will surely be noisier than indoor ones. 


\paragraph{}Depending on the application domain, some or all the invariances can be required for the comparison of time series. The preprocessing step can handle some of those invariances; for instance, z-normalization of time series allows their comparison to be scaling invariant. However, all invariances cannot be handled by preprocessing step and should then considered by more sophisticated distances or dissimilarities functions. In the next section, we review the most common of such distance measures.





\subsection{Categories of time series similarity function}


Time series similarity measures can be generally divided into following four main categories:

\paragraph{}\textbf{Shape Based similarity function} compares two time-series based on the sum of the distances in a Euclidian space between data points of both times series located at approximately the same timestamp. By doing so, the distance between two time series with similar shapes will be low. On the opposite, the distance between time-series that have a different shapes will be high. In this family, there are Lp norm \cite{yi2000fast} \cite{keogh2003need}, and Dynamic time warping distance \cite{ MyersRabinerRosenberg1980} for instance. However, those distances are sensitive to noise.

\paragraph{}\textbf{Edit Based distance} allows evaluating the dissimilarity between two character strings. These dissimilarity functions are able to handle noisy regions and outliers. The principle of these similarity functions is the following: Edit based distances count the minimum number of operation necessary to transform a character string to another. Different edit based dissimilarity functions use different operations to transform one string to another. A well known edit based distance is Levenshtein distance. That uses three operations: suppression, insertion and substitution of letters. Edit based distances in time series domain are based on the same principle. Time series data points can be skipped during the comparison (deletion) or one  data point can be compared to several data points of the other time series (insertion). Among the well-known  edit based distances in time series domain we can cited: Longest Common SubSequence (LCSS) \cite{das1997finding}, Edit Distance on Real sequence (EDR) \cite{chen2005robust} and Time Warp Edit Distance (TWED) \cite{marteau2009time} algorithms. LCSS distance uses a threshold parameter for point matching as well as a warping threshold for allowing gaps for matching two time series. EDR is a variant of the edit distance for real-valued series. Opposite to LCSS, EDR assigns penalties based on the length of existing gaps between two series. TWED is a dynamic programming algorithm that introduces a parameter to control the elasticity measure along the time axis. 

\paragraph{Feature Based distance:} this distance has been designed to ensure some invariances such as rotation invariance. Time series can be compared based on their properties rather than on their shape. So, Feature Based similarity measures compare two time series by computing a feature set for
each time series that reflects their properties\footnote{We will use this type of distance later when analyzing manual wheelchair locomotion (Chap. \ref{chapter_saxp}).}. For example, DFT and DWT coefficients can be used to compare the similarity between time series \cite{shatkay1996approximate}.


\paragraph{Structure Based similarity measures:}These measures are designed to compare time series on a global scale based on their structure. The general principle of those similarity functions is to compare time series based on a high-level representation that captures global properties of the time series, such as histogram, for instance  \cite{lin2009finding}.   



\section{Datamining task on time series}
\paragraph{Indexing time series: }
The problem of indexing or query by content can be defined as follows: given a query time series Q, and some similarity/dissimilarity measure D(Q,C), find the most similar time series in database DB. When querying time series by content, a challenge consists in finding as fast as possible a time series in the database that is similar to the query. To achieve this goal, some dimensionality reduction technics have been used:  for instance, in \cite{Agrawal1993}, time series have been transformed into a more compact representation using Discret Fourier Transform (DFT) before their comparison. Many other dimensionality reduction techniques have been used for the same purpose, such as Discrete Wavelet Transform (DWT) and Discrete Cosine Transform (DCT) \cite{chan1999efficient}. Other representation approaches used for query by content are PLA, PAA, APCA \cite{keogh2001locally}, and SAX \cite{Lin2007}. These latter authors \cite{Lin2007}  have shown that SAX outperforms other representations for query by content applications. Another strategy used to ensure the effectiveness of research is parallelism \cite{yagoubi2017radiussketch}.

\paragraph{Motif Discovery:}
Time series motifs are pairs of individual time series, or subsequences of a longer time series, which are very similar to each other and carry precise information about the underlying source of the time series. The idea for motif discovery in time series is inspired from DNA analysis. When they exist, motifs can be used to construct meaningful clusters when clustering time series, which is the case of unsupervised shapelet algorithm \cite{ulanova2015scalable}. Associating each class with a motif can speed-up the classification of time series; this idea is used by the shapelet transform algorithm\footnote{We will use this type of distance later when analyzing manual wheelchair locomotion (Appendix  \ref{training_technic} ).} \cite{lines2012shapelet}, \cite{yagoubi2018parcorr}.

\paragraph{Anomaly Detection:}
Anomaly detection refers to the problem of finding patterns in data that do not conform to the expected behavior. These nonconforming patterns are often referred to as anomalies, outliers, discordant observations, exceptions, aberrations, surprises, peculiarities, or contaminants in different application fields. Among these, anomalies and outliers are two terms  most commonly used in the context of anomaly detection; sometimes interchangeably. Anomaly detection finds extensive use in a wide variety of applications such as fraud detection for credit cards, insurance, or healthcare, intrusion detection for cyber-security, fault detection in safety-critical systems, and military surveillance of enemy activities \cite{chandola2009anomaly}.

\paragraph{Temporal Association Rule Discovery:}
In a transactional database, association rules allow searching for items that often appear together in the same transaction. For instance, in the database of a shop, the discovered rules will indicate, which products are often bought together. The association rules do not give any information on the precedence of the occurrence of one event concerning the other. Hence the need to define temporal association rules,  which are particularly appropriate as candidates for causal rules' analysis in temporally adorned medical data, such as in the histories of patients' medical visits. Patients are associated with both static properties, such as gender, and temporal properties, such as age or current medical treatments, any or all of which may be taken into account during mining \cite{Vasimalla2017}.

\paragraph{Summarization (Visualization):}
The problem of time series visualization or summarization can be defined as follows: given a time series $Q$ containing $n$ data points where, $n$ is an extremely large number, create a (possibly graphics) approximation of $Q$, which retains its essential features but fits on a single page, computer screen, executive summary. Summarization can be viewed as a higher level clustering of time series  where clusters are associated with text or graphical descriptions. Some famous approaches of time series summarization  are:
\begin{itemize}

\item    \textbf{Time searcher}: it is a query by content summarization tool. Here, a user specifies a set of constraints (intervals) graphically to which time series data points should belong. Those constraints are called time series boxes \\ \cite{hochheiser2003interactive}.
\item    \textbf{Calendar based visualization} of univariate time series data: its goal is to simultaneously identify patterns and trends on multiple time scales (days, weeks, seasons). To do so, Calendar based visualization first clusters similar daily data patterns and visualizes the average patterns as graphs and the corresponding days on a calendar \cite{van1999cluster}. 
\item    The \textbf{spiral visualization} is appropriated with large data sets and supports much better than line graphs the identification of periodic structures in the data. Spiral visualization supports both the visualization of nominal and quantitative data based. The extension of the spiral visualization to 3D gives access to concepts for zooming and focusing and linking in the data set \cite{weber2001visualizing}. 
\item    \textbf{GrammarViz} is a visualization tool that allows efficient discovery of frequent and rare patterns  of variable length in time series. It is based on the symbolic representation of time series SAX and context-free grammar \cite{senin2014grammarviz}.
\end{itemize} 

\paragraph{Prediction} or time series forecasting is one of the most useful data mining tasks on time series: for example, time series forecasting is used to predict the weather, the cost of an action in the stock exchange market, or early identified epidemiological risks and raised up alarms. A time series forecasting method is based on a mathematical model that capture the main characteristics of the time series like seasonality, periodicity, trend, and that can be used to guess unknown (or future) values of the time series. Many other algorithms used for time series forecasting are based on Auto-Regressive (AR) models. More sophisticated approaches are also used such as neural networks and cluster function approximation \cite{mahalakshmi2016survey}.
\paragraph{Classification:}
Classifying time series consists of assigning an unlabelled time series to one, two or more classes. Many classification algorithms for time series have been proposed in the literature and can be gathered into four main groups
\begin{itemize}
\item \textbf{Dictionary classifiers}: generally, these classifiers first transform time series into characters strings that can be decomposed into a set of word or bag of words, a word being simply a subsequence of the characters string. Each time series is then described by the occurrence frequency of each word in it. The set of time series represented in the space of words is called a dictionary; The classification of time series is then based on the presence or absence of words in this dictionnary. Several algorithms of the literature are based on this principle, such as Bag of Patterns \cite{lin2012rotation},  SAX and Vector Space Model \cite{senin2013sax}, Bag of SFA Symbols(BOSS) \cite{schafer2015boss}, DTW Features \cite{kate2016using}, temporal-robust text classification \cite{salles2017two} .

\item \textbf{Classifier-based on the alignment of whole time series}: those classifiers are based on distance functions that operate over the entire length of the time series. The difference between the classifiers of this family is based in part on the characteristics of the distance functions used. These distance functions can be based on the shape of the time series (Derivative Dynamic Time Warping  \cite{keogh2001derivative}, Weighted Dynamic Time Warping  \cite{jeong2011weighted}, Complexity-Invariant Distance  \cite{batista2011complexity}), on their properties, on their structures or on their symbolic representation (Time Warp Edit Distance \cite{marteau2008time}, Move Split Merge \cite{stefan2013move}).

\item \textbf{Shapelets Classifiers:}  Unlike classifiers based on the comparison of the time series over their entire length, shapelets classifiers look for characteristic sub-sequences in time series called shapelets whose presence or absence indicates whether or not a time series belongs to a class. We have for example: Shapelet Transform \cite{lines2012shapelet}, Learned Shapelets \cite{grabocka2014learning}, Fast Shapelet Tree \cite{rakthanmanon2013fast}

\item \textbf{Intervals Classifiers:} The idea here is to find localized discriminatory features on time series based on some statistical properties calculated over intervals of variable length. A time series of length $m$ will have $ m(m-1)/2$ possible contiguous intervals. An interval associated with some statistical properties and a condition is a literal, which gives some information about what happened in an interval: for instance, is the mean of a sequence of data points greater or less than a define threshold? The classifier tries to find a relationship between what happened in an interval and  time series classes. Many classifiers are based on this principle, such as: Time Series Bag of Features  \cite{baydogan2013bag}, Time Series Forest  \cite{deng2013time}, Learned Pattern Similarity  \cite{baydogan2016time}.
\end{itemize}


\paragraph{Clustering:}
The clustering of time series consists of grouping them to build very homogeneous and well-separated groups under some similarity/dissimilarity measure D(Q, C) \cite{rani2012recent}. "Homogeneous" means that the intra-group variance is small and "Well separated" means that the inter-groups variance is high. There are many ways to categorize time series clustering algorithms depending on the \textbf{distance function} used, the \textbf{data transformation} or  the \textbf{clustering strategy}. 


When considering \textbf{distance function}, we have two categories of time series clustering algorithms: those that operate on the whole time series and those that operates on a sub-sequence of time series.


 When considering \textbf{data transformation}, we can  gather time series clustering algorithms into three groups:  raw data, feature-based and model-based clustering. 


When considering \textbf{clustering strategy}, we have five categories of clustering algorithms: 

\begin{itemize}
\item   \textbf{Distance-based} clustering, which is divided in two sub-categories:
	\begin{itemize}
	\item \textbf{Partitioning clustering algorithms}  partition the data in        	high dimensional space into multiple clusters. We have for example kMeans like 		algorithms (kMeans, kMedians, kMedoids, XMeans, KMeans++)
	\item \textbf{Hierarchical clustering algorithms} are grouped into two 				subcategories: \textbf{Agglomerative clustering algorithms} first consider each 		object of the dataset as a cluster and then try to merge clusters until  obtainning 		one cluster: it is a bottom-up merging strategy. \textbf{Divisive clustering 			algorithms} first considers that all the data points are in the same cluster 		and then try to split this cluster to obtain more homogenous ones: it is the top-down merging strategy.
	\end{itemize}
   
\item \textbf{Density-Based clustering and grid-based clustering algorithm}:
 
\begin{itemize}
	\item The principle of \textbf{density-based clustering} is the following:   given a time 		series that will be considered as the center of the cluster,  all the time series of the database that have a distance less or 		equal to a defined threshold to the center of the cluster are gathered. Thus, the 			algorithm splits the space into more or less dense regions; then small dense  			regions can be merged into more significant regions. This algorithm allows to identify  		clusters of arbitrary shapes  \cite{kriegel2011density}.
	\item \textbf{Grid-based clustering} divides the data space into a grid-like 				structure, which allows determining the characteristics of the data \cite{amini2011study}.
\end{itemize}


\item \textbf{Probabilistic and generative models} can be modelled  with a generative process assuming the data follow a particular distribution like a mixture of Gaussian. Then the model parameters are estimated using the expectation-maximization algorithm (EM), that considers the parameters that maximize the likelihood of the model to the data. On this basis we may estimate the generative probabilities that will be used to construct the generative model \cite{merugu2003privacy}.


\item  \textbf{High-dimensional clustering algorithms}: time series may be set in a high dimensional feature space. To cluster them, many methods have been proposed:

\begin{itemize}
	\item  \textbf{Subspace clustering}: Subspace clustering looks for a cluster in 	different subspaces of a dataset.  A subspace is a subset of the $d$ dimensions 		of a given dataset; this algorithm is used because, generally, all the dimensions of high dimensional data are not useful. 	Subspace clustering algorithm identifies the relevant dimensions that allow 		finding clusters. There are two main subspace clustering branches based on their 		search strategy. Top-down algorithms find an initial clustering in the full set 		of dimensions and evaluate the subspaces of each cluster, iteratively improving 	the results. Bottom-up approaches find dense regions in low dimensional spaces 		and combine them to form clusters \cite{parsons2004subspace}.
	\item \textbf{Dimensionality reduction}: many dimensionality reduction 				techniques have been proposed for clustering purpose. A well-known one is co-clustering, which consists of simultaneously clustering  columns (or dimensions) 	and	rows (data points) of a matrix \cite{dhillon2003information}.
	\item \textbf{Probabilistic latent semantic indexing (PLSI)} and \textbf{laten 		dirichlet allocation (LDA)}  are typical clustering techniques for text data. Indeed, text can be clustered in multiple topics and  each topic can be associated with a 		set of words (or dimension) or a set of rows (documents) \cite{hofmann2017probabilistic}.
	\item \textbf{Nonnegative matrix factorization} is a kind of co-clustering 			algorithm. It proceeds as follows: a nonnegative matrix $X \in \mathbb{R}^{M			\times N}$ can be 			factorised into two lower rank matrices $U \in 			\mathbb{R}^{M\times L}$ and $V \in \mathbb{R}^{L\times N}$ with $L < M$ and $L 		< N$.  The idea here is to identify clusters using the matrix $U$, which has a lower 	dimension than $X$  						\cite{wang2013nonnegative}.
	\item \textbf{Spectral clustering}: the principle is to cluster time series or 		data objects based on the spectrum of their similarity matrix. The spectrum 			is used here for dimensionality reduction  \cite{filippone2008survey}.
	
\end{itemize}
 

\end{itemize}
In the context of this thesis, the analysis of manual wheelchair locomotion implies to group  manual wheelchair users with similar motor abilities, based on measurements made during their locomotion in actual conditions: this approach is  equivalent to time series clustering. Several clustering algorithms proposed in the literature are harmonious compositions, consisting of a representation of time series, an adequate distance function and an appropriate clustering strategy as illustrated  the Tables \ref{tab:1}, \ref{tab:2} and \ref{tab:3}. Detailed informations is presented in \cite{rani2012recent}. 

 
 
\begin{landscape}
% Please add the following required packages to your document preamble:
% \usepackage[normalem]{ulem}
% \useunder{\uline}{\ul}{}
\begin{table}[ht]
\centering
\small
\caption{Temporal-Proximity-Based Clustering Approach}
\label{tab:1}
\begin{tabular}{|l|l|l|l|}
\hline
\multicolumn{1}{|c|}{\textbf{Author}}                           & \multicolumn{1}{c|}{\textbf{Distance Measure}}                                                                                               & \multicolumn{1}{c|}{\textbf{Algorithm}}                                                  & \multicolumn{1}{c|}{\textbf{Application}}                                                       \\ \hline
M. Kumar                                                       & \begin{tabular}[c]{@{}l@{}}Based      on      the      assumed  independent   \\   Gaussian models   of\\   data errors\end{tabular} & Agglomerative Hierarchical                                                               & Seasonality pattern in retails                                                                  \\ \hline
T.-W. Liao                                                     & \begin{tabular}[c]{@{}l@{}}Euclidean      and    symmetric\\   version    of    Kullback–Liebler\\   distance\end{tabular}             & \begin{tabular}[c]{@{}l@{}}K-Means\\   and Fuzzy C-Means\end{tabular}                    & Battle simulations                                                                              \\ \hline
T.-W. Liao                                                     & Dynamic Time Warping                                                                                                                         & \begin{tabular}[c]{@{}l@{}}K-Medoids    Based     Genetic\\   Clustering\end{tabular} & Battle simulations                                                                              \\ \hline
C.S. Möller-Levet                                              & \begin{tabular}[c]{@{}l@{}}Short     time     series     (STS)   distance\end{tabular}                                                     & Modified Fuzzy C-Means                                                                   & DNA microarray                                                                                  \\ \hline
Shumway                                                        & \begin{tabular}[c]{@{}l@{}}Kullback–Leibler\\   discrimination         information measure\end{tabular}                                      & Agglomerative Hierarchical                                                               & \begin{tabular}[c]{@{}l@{}}Earthquakes     and     mining  explosions\end{tabular}   \\ \hline
Vit Niennattrakul                                              & Dynamic Time Warping                                                                                                                         & K-Means, K-Medoids                                                                       & \begin{tabular}[c]{@{}l@{}}Multimedia time\\   series\end{tabular}                              \\ \hline
\begin{tabular}[c]{@{}l@{}}Pooya Sobhe\\   Bidari\end{tabular} & Pearson Correlation                                                                                                                          & K-Means, Fuzzy C-Means                                                                   & Pattern extraction in genes                                                                     \\ \hline
Hardy Kremer                                                   & Dynamic Time Warping                                                                                                                         & \begin{tabular}[c]{@{}l@{}}Density   Based \\    Subsequence\\   Clustering\end{tabular} & \begin{tabular}[c]{@{}l@{}}Detecting climate\\   change\end{tabular}                            \\ \hline
Jian Yin                                                       & Grey Relation                                                                                                                                & Hierarchical Clustering                                                                  & \begin{tabular}[c]{@{}l@{}}Change  trend  of\\    traffic  flow  data\end{tabular}       \\ \hline
S. Chandrakala                                                 & Euclidean                                                                                                                                    & Kernal DBScan                                                                            & \begin{tabular}[c]{@{}l@{}}Multivariate       time         series\\   clustering\end{tabular} \\ \hline
Aurangzeb Khan                                                 & Euclidean                                                                                                                                    & \begin{tabular}[c]{@{}l@{}}K-Mean+ MFP(Most Frequent\\   Pattern)\end{tabular}           & Stock and inventory data                                                                        \\ \hline
Mengfan Zhang                                                  & \begin{tabular}[c]{@{}l@{}}CVT(Computational          Verb\\   Theory)\end{tabular}                                                          & K-Means                                                                                  & Stock market data                                                                               \\ \hline
S.R.Nanda                                                      & Euclidean                                                                                                                                    & K-Means                                                                                  & Portfolio management                                                                            \\ \hline
Jianfei Wu                                                     & N/A                                                                                                                                          & K-Means                                                                                  & Stock data                                                                                      \\ \hline
\end{tabular}
\end{table}
\end{landscape}



\begin{landscape}
\begin{table}[ht]
\centering
\small
\caption{Representation-Based Clustering Approach Paper}
\label{tab:2}
\begin{tabular}{|l|l|l|l|l|}
\hline
\multicolumn{1}{|c|}{\textbf{Author}} & \multicolumn{1}{c|}{\textbf{Features}}                                     & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Distance\\   Measure\end{tabular}}}                                    & \multicolumn{1}{c|}{\textbf{Clustering Algorithm}}                                 & \multicolumn{1}{c|}{\textbf{Application}}                                              \\ \hline
T.-C. Fu                             & \begin{tabular}[c]{@{}l@{}}Perceptually important\\   points\end{tabular}  & \begin{tabular}[c]{@{}l@{}}Sum of the mean\\   squared distance along the\\   vertical and horizontal\\   scales\end{tabular} & Modified SOM                                                                       & Hong Kong stock market                                                                 \\ \hline
M. Vlachos                           & \begin{tabular}[c]{@{}l@{}}Haar wavelet\\   transform\end{tabular}         & Euclidean                                                                                                                     & Modified k-means                                                                   & Non-specific                                                                           \\ \hline
Huiting Liu                          & \begin{tabular}[c]{@{}l@{}}Empirical mode\\   decomposition\end{tabular}   & Euclidean                                                                                                                     & \begin{tabular}[c]{@{}l@{}}Forward propagation\\   learning algorithm\end{tabular} & Non-specific                                                                           \\ \hline
Chonghui GUO                         & \begin{tabular}[c]{@{}l@{}}Independent component\\   analysis\end{tabular} & Euclidean                                                                                                                     & Modified k-means                                                                   & \begin{tabular}[c]{@{}l@{}}Real world stock time \\   series\end{tabular}              \\ \hline
Jian Xin Wu                          & \begin{tabular}[c]{@{}l@{}}Independent component\\   analysis\end{tabular} & N/A                                                                                                                           & \begin{tabular}[c]{@{}l@{}}support vector\\   regression\end{tabular}              & Financial time series                                                                  \\ \hline
Geert Verdoolaege                    & Wavelet transform                                                          & \begin{tabular}[c]{@{}l@{}}Kullback- Liebler\\   divergence\end{tabular}                                                      & k-means                                                                            & \begin{tabular}[c]{@{}l@{}}Detection of activated\\   voxels in FMRI data\end{tabular} \\ \hline
Liu Suyi                             & Hough transform                                                            & N/A                                                                                                                           & Mean shift algorithm                                                               & \begin{tabular}[c]{@{}l@{}}Feature recognition of\\   underwater images\end{tabular}   \\ \hline
Dong Jixue                           & Wavelet transform                                                          & N/A                                                                                                                           & \begin{tabular}[c]{@{}l@{}}Grid-based partitioning\\   method\end{tabular}         & Financial time series                                                                  \\ \hline
\end{tabular}
\end{table}
\end{landscape}


\begin{landscape}
\begin{table}[ht]
\centering
\small
\caption{Model-Based Clustering Approach}
\label{tab:3}
\begin{tabular}{|l|l|l|l|l|}
\hline
\multicolumn{1}{|c|}{\textbf{Author}} & \multicolumn{1}{c|}{\textbf{Model}}                                                     & \multicolumn{1}{c|}{\textbf{Distance measure}}                      & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Clustering\\   algorithm\end{tabular}}} & \multicolumn{1}{c|}{\textbf{Application}}                                              \\ \hline
Baragona                             & ARMA                                                                                    & \begin{tabular}[c]{@{}l@{}}Cross-correlation\\   based\end{tabular} & \begin{tabular}[c]{@{}l@{}}Tabu    search,  \\    GA\\   and\end{tabular}                      & Non-specific                                                                           \\ \hline
K. Kalpakis                          & AR                                                                                      & Euclidean                                                           & Partitioning   around medoids                                                                  & Public data                                                                            \\ \hline
Xiong and Yeung                      & ARMA mixture                                                                            & Log-likelihood                                                       & EM learning                                                                                    & Public data                                                                            \\ \hline
L. Wang                              & Discrete HMM                                                                            & Log-liklihood                                                       & EM learning                                                                                    & \begin{tabular}[c]{@{}l@{}}Tool        \\    condition\\   monitoring\end{tabular}     \\ \hline
Xin Huang                            & \begin{tabular}[c]{@{}l@{}}Fuzzy  set\\    and\\    R/S\\   analysis model\end{tabular} & N/A                                                                 & \begin{tabular}[c]{@{}l@{}}Fuzzy     \\    clustering iteration method\end{tabular}            & \begin{tabular}[c]{@{}l@{}}Predicting\\   agriculture drought\end{tabular}             \\ \hline
Shan Gao                             & ARMA-ARCH                                                                               & N/A                                                                 & N/A                                                                                            & \begin{tabular}[c]{@{}l@{}}To analyze the\\   effects of wind data series\end{tabular} \\ \hline
\end{tabular}
\end{table}
\end{landscape}


\section{Conclusion}
Time series are ubiquitous in science and are more and more used in the analysis of human locomotion. This chapter presents a general framework for extracting knowledge from time series, starting by time series pre-processing, which allows reducing the advert effects of noise, uncertainty, and dimensionality. Then, we present strategies that are used to compare time series, and we finally present data mining tasks on time series. This chapter presents what has been already done in the literature and raises the question of extracting relevant information from time series coming from wheelchair locomotion. The following chapters present some new strategies that are adapted to the characteristics of these particular time series.


\begin{table}[ht]
\centering
\begin{tabular}{|p{15cm}|}

\hline
\rowcolor{LavenderBlush}
Key points\\
We highlight  different models of data mining  that could  be useful to our work, particularly for  \\
\\
$\bullet$ Pre-processing of time series (noise reduction, dimensionality reduction) \\
$\bullet$ Comparison of time series.\\ 
$\bullet$ Different data mining tasks performed on time series.\\
\hline
\end{tabular}
\end{table}
